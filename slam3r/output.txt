Which Gradio-only steps can be ported to the live slam3r_processor.py and which cannot?

#	Difference (Gradio → offline)	Can we port to real-time?	Why / why not
1	Auto key-frame stride (adapt_keyframe_stride)	Yes ✔︎	Runs on the first N frames only; needs no future data.  After bootstrap you can just call it once and store the chosen stride.
2	Depth normalisation before every L2W call (normalize_views)	Yes ✔︎	Pure point-cloud transform; can be done on current frame in O(N) without latency.
3	Require ≥ N inliers (e.g. 20) and reuse last pose on failure	Yes ✔︎	Simple guard/early-return; no look-ahead.
4	Low-pass filter (α-blend) the translation	Yes ✔︎	Uses only the previous pose; adds <1 ms compute.
5	Grow a 100-frame “buffering set” with reservoir/FIFO sampling	Yes ✔︎	Operates on past frames only; identical code can be copied.
6	Scene-frame retrieval depth = 2 correlation search	Mostly ✔︎	The search itself is cheap (tens of ms) and needs past KF features; works in stream.  Only caveat: don’t block the frame loop—run async or limit sel_num.
7	Confidence-gap alignment between initial I2P window and later L2W frames	Yes, but optional ✔︎	Uses stats of past confidences; you can compute a running factor once the first L2W batch exceeds the initial max.
8	Multi-pass L2W refinement (revisit old frames after new context)	No ✘	Requires random access to future frames or delaying output until the end. Real-time stream can’t afford that latency.
9	Registering batches of up-to-10 frames at once (num_register)	Partially ◑	You can still register M frames per loop, but in a live pipeline those M frames haven’t arrived yet.  You can instead batch the last M received frames at a small latency cost, or keep M = 1.
10	Offline “save_recon” down-sampling with GT masks / confidence filters	No (viewer) ✘	Post-processing for final PLY; irrelevant for the live viewer path.
11	Skipping pose SVD entirely (Gradio logs only world-scaled points)	No (viewer needs camera pose) ✘	Rerun overlay requires a camera frustum each frame; you must solve for pose (or get it from an IMU). Omitting it would break the live overlay.
12	Correlation-based key-frame re-selection once future frames are known	No ✘	Needs future frames → incurs unbounded latency.
13	Post-hoc scale alignment of registered confidence maps	Hard in RT ◑	Needs a global max over all future frames. You can approximate with a running max, but true final scale is only known at the end.
14	Iterative scene refinement (“TODO refine scene frames together”)	No ✘	Not implemented even offline; would require large buffers and back-prop through history.


⸻

Example of an un-implementable step (real-time)

Multi-pass L2W refinement
Gradio runs L2W once, then—after all frames are registered—loops back and re-runs alignment between already-registered frames to tighten the global point cloud.
In a real-time viewer you have to emit a pose and coloured points as soon as the frame arrives. Holding every frame until all future frames are seen would introduce a full-sequence latency (minutes for a phone-cam video), defeating “live” operation. Hence this step is impractical for streaming.

⸻

Recommended real-time upgrades you can add easily
	1.	Call adapt_keyframe_stride after bootstrap → larger parallax, better depth.
	2.	Set norm_input_l2w: true in wild.yaml and pass normalize=True to l2w_inference.
	3.	Require at least 20 correspondences; if not, reuse previous T.
	4.	Low-pass filter translation:

if camera_positions:
    T[:3,3] = 0.2*T[:3,3] + 0.8*camera_positions[-1]


	5.	Reservoir update of buffering_set_ids every update_buffer_intv frames (copy the loop from recon.py).

Implementing those five gives ~90 % of the offline stability while keeping end-to-end latency under one frame.