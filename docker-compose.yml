services:
  rabbitmq:
    image: rabbitmq:4-management          # 4.1 with the Management UI
    container_name: rabbitmq
    network_mode: host
    environment:
      RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS: >-
        -rabbitmq_prometheus.listener.tcp 0.0.0.0 15692
        -rabbit frame_max 1048576
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq   # persist queues/users
    
  nginx:
    build: ./nginx
    network_mode: host
    depends_on:
      - server
  
  server:
    image: server:latest
    build:
      context: ./server
      args:
        USERNAME: ${USERNAME}
        UID: ${UID}
        GID: ${GID}
    network_mode: host
    volumes:
      - ${WORKSPACE}/server:/app
      - ${X11SOCKET}:${X11SOCKET}
      - ${XAUTHORITY}:${XAUTHORITY}
    environment:
      - RABBITMQ_URL=amqp://127.0.0.1:5672
      - VIDEO_FRAMES_EXCHANGE=video_frames_exchange
      - IMU_DATA_EXCHANGE=imu_data_exchange
      - PROCESSED_FRAMES_EXCHANGE=processed_frames_exchange
      - ANALYSIS_MODE_EXCHANGE=analysis_mode_exchange
      - RESTART_EXCHANGE=restart_exchange
    depends_on:
      - rabbitmq

  frame_processor:
    build: ./frame_processor
    image: frame_processor:latest
    runtime: nvidia
    network_mode: host
    environment:
      - RABBITMQ_URL=amqp://127.0.0.1:5672
      - VIDEO_FRAMES_EXCHANGE=video_frames_exchange
      - IMU_DATA_EXCHANGE=imu_data_exchange
      - PROCESSED_FRAMES_EXCHANGE=processed_frames_exchange
      - ANALYSIS_MODE_EXCHANGE=analysis_mode_exchange
      - INITIAL_ANALYSIS_MODE=none #yolo

      # Added Rerun configuration
      - RERUN_VIEWER_ADDRESS=0.0.0.0:9090
      - RERUN_CONNECT_URL=rerun+http://127.0.0.1:9876/proxy
      - RERUN_ENABLED=true

      # Prometheus metrics port
      - METRICS_PORT=8003

      # GPU / display environment variables
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_PATH=${CUDA_PATH}
      - LIBGL_ALWAYS_INDIRECT=${LIBGL_ALWAYS_INDIRECT}
      - DISPLAY=${DISPLAY}
      - HOME=${HOME}
    volumes:
      - ${WORKSPACE}/frame_processor:/app
      - ${X11SOCKET}:${X11SOCKET}
      - ${XAUTHORITY}:${XAUTHORITY}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      - rabbitmq

  data_storage:
    image: data_storage:latest
    build: ./storage
    volumes:
      - ${WORKSPACE}/storage:/app
      - ${WORKSPACE}/data:/data
    network_mode: host
    # RabbitMQ now reached on the host‑published port
    environment:
      RABBITMQ_URL: amqp://127.0.0.1:5672
      VIDEO_FRAMES_EXCHANGE: video_frames_exchange
      IMU_DATA_EXCHANGE: imu_data_exchange
      TRAJECTORY_DATA_EXCHANGE: trajectory_data_exchange
      PLY_FANOUT_EXCHANGE: ply_fanout_exchange
      RESTART_EXCHANGE: restart_exchange
      PROCESSED_IMU_EXCHANGE: processed_imu_exchange
    depends_on:
      - rabbitmq

  slam3r:
    image: slam3r:latest
    privileged: true
    build:
      context: ./slam3r
      args:
        USERNAME: ${USERNAME}
        UID: ${UID}
        GID: ${GID}
    container_name: slam3r
    runtime: nvidia
    pid: host
    cap_add:
     - SYS_PTRACE

    # Mount source and checkpoints
    volumes:
      - ${WORKSPACE}/slam3r/SLAM3R_engine/slam3r_processor.py:/app/SLAM3R_engine/slam3r_processor.py
      - ${WORKSPACE}/slam3r/SLAM3R_engine/configs:/app/SLAM3R_engine/configs
      - ${WORKSPACE}/slam3r/checkpoints:/checkpoints_mount
      - ${X11SOCKET}:${X11SOCKET}
      - ${XAUTHORITY}:${XAUTHORITY}

    # ---------- key change: share the host network namespace ----------
    network_mode: host

    # ========== MEMORY AND RESOURCE MANAGEMENT ==========
    # Shared memory for PyTorch (important for multi-process data loading)
    shm_size: '4gb'  # Reduced from 8gb since GPU is shared
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Health check to detect and recover from OOM
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; assert torch.cuda.is_available(); assert torch.cuda.memory_allocated() < 20e9"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Auto-restart on crashes (including OOM)
    restart: unless-stopped

    # ‼️ host‑network means container DNS can't resolve other service names; connect via 127.0.0.1 instead
    environment:
      # ========== EXISTING CONFIGURATION ==========
      # RabbitMQ now reached on the host‑published port
      - RABBITMQ_URL=amqp://127.0.0.1:5672

      # Input exchanges
      - VIDEO_FRAMES_EXCHANGE=video_frames_exchange
      - TRAJECTORY_DATA_EXCHANGE=trajectory_data_exchange
      - PLY_FANOUT_EXCHANGE=ply_fanout_exchange
      - RESTART_EXCHANGE=restart_exchange

      # Output exchanges
      - SLAM3R_POSE_EXCHANGE=${SLAM3R_POSE_EXCHANGE:-slam3r_pose_exchange}
      - SLAM3R_POINTCLOUD_EXCHANGE=${SLAM3R_POINTCLOUD_EXCHANGE:-slam3r_pointcloud_exchange}
      - SLAM3R_RECONSTRUCTION_VIS_EXCHANGE=${SLAM3R_RECONSTRUCTION_VIS_EXCHANGE:-slam3r_reconstruction_vis_exchange}
      - SLAM3R_OUTPUT_TO_RABBITMQ=${SLAM3R_OUTPUT_TO_RABBITMQ:-false}

      # Paths & checkpoints
      - SLAM3R_CHECKPOINTS_DIR=/checkpoints_mount
      - SLAM3R_CONFIG_FILE=/app/SLAM3R_engine/configs/wild.yaml
      - CAMERA_INTRINSICS_FILE=/app/SLAM3R_engine/configs/camera_intrinsics.yaml

      # Rerun viewer (host‑local)
      - RERUN_VIEWER_ADDRESS=127.0.0.1:9090
      - RERUN_CONNECT_URL=rerun+http://127.0.0.1:9876/proxy
      - RERUN_ENABLED=${RERUN_ENABLED:-true}

      # GPU / display
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_PATH=${CUDA_PATH}
      - LIBGL_ALWAYS_INDIRECT=${LIBGL_ALWAYS_INDIRECT}
      - DISPLAY=${DISPLAY}

      # ========== MEMORY MANAGEMENT (CRITICAL FOR 3090) ==========
      # PyTorch memory allocation strategy - VERY IMPORTANT
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:256,garbage_collection_threshold:0.7
      
      # Enable CUDA memory debugging in case of issues
      - CUDA_LAUNCH_BLOCKING=${CUDA_LAUNCH_BLOCKING:-0}
      
      # Frame history and tensor cache limits (conservative for shared GPU)
      - SLAM3R_MAX_HISTORY_SIZE=300  # Reduced from 500
      - SLAM3R_MAX_TENSOR_CACHE=50   # Reduced from 100
      - SLAM3R_MAX_POINTCLOUD_SIZE=750000  # Reduced from 1M
      
      # Force garbage collection interval
      - SLAM3R_GC_INTERVAL=30  # seconds

      # ========== VISUALIZATION OPTIMIZATION ==========
      # Batch processing for Rerun to reduce overhead
      - SLAM3R_RERUN_BATCH_SIZE=15  # Increased for better batching
      - SLAM3R_RERUN_VOXEL_SIZE=0.008  # Slightly larger for performance
      - SLAM3R_RERUN_KEYFRAME_VOXEL_SIZE=0.005  # Better quality for keyframes
      
      # Limit visualization updates
      - SLAM3R_VIZ_UPDATE_INTERVAL=5  # Only update viz every N frames

      # ========== ADAPTIVE TRACKING FOR CORRIDORS ==========
      # Scene detection and adaptive keyframing
      - SLAM3R_SCENE_DETECTION_ENABLED=true
      - SLAM3R_CORRIDOR_DETECTION_WINDOW=20
      - SLAM3R_CORRIDOR_EIGENVALUE_RATIO=5.0
      
      # Keyframe adaptation parameters
      - KEYFRAME_STRIDE=-1  # Auto-adaptive
      - SLAM3R_KEYFRAME_ADAPT_MIN=1
      - SLAM3R_KEYFRAME_ADAPT_MAX=15  # Reduced max for better tracking
      - SLAM3R_KEYFRAME_ADAPT_STRIDE_STEP=1
      
      # Buffer strategies
      - SLAM3R_BUFFER_STRATEGY=${SLAM3R_BUFFER_STRATEGY:-reservoir}  # Default
      - SLAM3R_BUFFER_SIZE=80  # Reduced for memory
      - SLAM3R_UPDATE_BUFFER_INTV=1
      
      # Corridor-specific thresholds
      - SLAM3R_CORRIDOR_POSITION_THRESHOLD=0.4  # meters
      - SLAM3R_CORRIDOR_ROTATION_THRESHOLD=12   # degrees
      - SLAM3R_ROOM_POSITION_THRESHOLD=0.8      # meters
      - SLAM3R_ROOM_ROTATION_THRESHOLD=25       # degrees

      # ========== SLAM3R ALGORITHM PARAMETERS ==========
      # Confidence thresholds (tuned for quality vs memory)
      - SLAM3R_CONF_THRES_I2P=1.8  # Slightly higher to reduce points
      - SLAM3R_CONF_THRES_L2W=14.0  # Higher threshold for cleaner results
      
      # Window and scene parameters
      - SLAM3R_INITIAL_WINSIZE=5
      - SLAM3R_WIN_R=4  # Slightly reduced
      - SLAM3R_NUM_SCENE_FRAME=8  # Reduced from 10
      - SLAM3R_MAX_NUM_REGISTER=8  # Reduced from 10

      # ========== PERFORMANCE TUNING ==========
      # Thread limits to prevent CPU oversubscription
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4
      - NUMEXPR_NUM_THREADS=4
      
      # Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app:$PYTHONPATH
      
      # Logging level
      - SLAM3R_LOG_LEVEL=${SLAM3R_LOG_LEVEL:-INFO}

      # ========== INITIALIZATION QUALITY ==========
      - INITIALIZATION_QUALITY_MIN_AVG_CONFIDENCE=1.2
      - INITIALIZATION_QUALITY_MIN_TOTAL_VALID_POINTS=150

      # ========== VIDEO SEGMENT HANDLING ==========
      # Save point clouds and trajectories when transitioning between segments
      - SLAM3R_SAVE_SEGMENT_POINTCLOUDS=${SLAM3R_SAVE_SEGMENT_POINTCLOUDS:-false}
      - SLAM3R_SEGMENT_OUTPUT_DIR=${SLAM3R_SEGMENT_OUTPUT_DIR:-/tmp/slam3r_segments}

      # ========== MISC ==========
      - HOME=${HOME}
      - TZ=${TZ:-UTC}

    depends_on:
      - rabbitmq
    
    # Resource usage logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  simulator:
    build: ./simulation  # or point to a Dockerfile that includes your simulation script
    profiles: ["simulator"]
    image: simulator:latest
    network_mode: host
    restart: "no"  # Don't restart when simulation completes
    volumes:
      - ${WORKSPACE}/data:/data  # mount the data folder so the script can access recorded data
      - ${USE_FOLDER:-/data}:/simulation_data  # mount specific folder for simulation (defaults to /data)
    environment:
      - PYTHONUNBUFFERED=1  # Ensures Python output is sent straight to the terminal
      - RABBITMQ_URL=amqp://127.0.0.1:5672
      - VIDEO_FRAMES_EXCHANGE=video_frames_exchange
      - IMU_DATA_EXCHANGE=imu_data_exchange
      - TRAJECTORY_DATA_EXCHANGE=trajectory_data_exchange
      - PLY_FANOUT_EXCHANGE=ply_fanout_exchange

      - USE_FOLDER=${USE_FOLDER:-/home/sam3/Desktop/Toms_Workspace/WorldSystem/20250617_211214_segments}  # set in .env file!!!
    depends_on:
      - rabbitmq

  website:
    image: website:latest
    build:
      context: ./website
      args:
        USERNAME: ${USERNAME}
        UID: ${UID}
        GID: ${GID}
    container_name: website
    network_mode: host
    volumes:
      - ${WORKSPACE}/website:/app
      - /app/node_modules
    environment:
      VITE_API_URL: "http://localhost/api"
      VITE_WS_HOST: "localhost"
      NODE_ENV: "development"
    depends_on:
      - rabbitmq
      - server

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    network_mode: host
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    depends_on:
      - cadvisor

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    network_mode: host
    depends_on:
      - prometheus
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana-data:/var/lib/grafana

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    network_mode: host
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    cpus: '0.2'

  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    network_mode: host
    environment:
      COLLECTOR_OTLP_ENABLED: "true"       # Enable OTLP ingestion
      COLLECTOR_OTLP_HTTP_HOST_PORT: ":4318"

  nvidia-dcgm-exporter:
    # use NVIDIA's canonical registry & newest tag
    image: nvcr.io/nvidia/k8s/dcgm-exporter:4.2.3-4.1.1-ubuntu22.04
    container_name: nvidia-dcgm-exporter
    runtime: nvidia

    # full set of GPU‑profiling metrics needs SYS_ADMIN
    cap_add:
      - SYS_ADMIN

    environment:
      - NVIDIA_VISIBLE_DEVICES=all        # all GPUs
    network_mode: host

volumes:
  grafana-data:
  rabbitmq-data:
